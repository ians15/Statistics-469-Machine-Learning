Final Project: Cell Type Annotation Using Machine Learning
In this final project, you will apply machine learning methods to solve a real-world problemâ€”cell type annotation using single-cell genomic data.

Data
You are provided with two datasets:

Training dataset: includes cell type labels for model training.

Test dataset: does not include cell type labels and is used for final predictions.

Submission Requirements
You must submit the following three files:

1. Report
Describe your methodology, including:

Model selection

Feature engineering

Evaluation strategies

Explain how the model was trained and how hyperparameters were selected.

Discuss any novel modifications or enhancements you implemented.

2. Code
R or Python code that is well-documented and capable of reproducing all results in your report.

3. RDS File of Predicted Labels
A single RDS file containing a vector of predicted labels for the test dataset.

The order of the cells must remain unchanged.

Do not include any additional information in this file.

Your predictions will be compared against true labels to calculate:

F1 score for each cell type

Average F1 score (used for ranking)

ðŸ§® Marking Scheme
1. Clarity and Rigor of Analysis (30%)
Clearly describe methods and analysis steps.

Include numerical results from training data (e.g., how hyperparameters were selected).

Justify model choices with sound reasoning and evaluation metrics.

2. Reproducibility of Code (40%)
Code must reproduce all report results and generate the submitted predictions. (35%)

Code must be clear, organized, and well-documented. (5%)

3. Novelty of Approach (10%)
Clearly describe your novel contribution(s). Novelty may include:

Modifying existing algorithms

Innovative feature engineering

Combining multiple models

Entirely new methodological approaches

Note: Simply using class methods or cross-validation does not count as novelty.

4. Performance (20%)
Based on ranking of average F1 scores on test data

Baseline: performance of default Random Forest (RF)

Grades distributed (example with 6 teams above baseline RF):
20, 18, 16, 14, 12, 10
Teams not beating baseline RF receive 0

You must:

Report RF performance on training data

Submit RF predictions in addition to your best modelâ€™s

ðŸ’¡ Additional Advice
Model Choice: Off-the-shelf models are fine, but adapt/combine them creatively if possible.

Novelty: Focus on enhancement opportunities (e.g., custom features, model stacking).

Code Quality: Structure clearly, include comments, and prioritize readability.

Teamwork: Collaborate effectively; evenly divide work and responsibilities.
